# Set up the environment
Create a virtual environment:
```bash
python3 -m venv .venv
```

Source the environment:
```bash
sourche .venv/bin/activawe
```

Source the environment on the server:
```bash
eval "$(/home/SA24-G1/miniconda3/bin/conda shell.bash hook)"
```

Install the requirements:
```bash
pip install -r requirements.txt
```

```bash
export PYTHONPATH="$PYTHONPATH:$PWD"
```

# Scrape the data 
The data is provvided and can be downloaded from [HERE](https://drive.proton.me/urls/MZMDKRAXM0#DPBSHmy8ATHp)

TODO command to scrape the data

# Pre-Process the data 
The raw Data is expected to be in the `data` directory. The name of the file is expected to be `fd.json`
Download the raw data from [HERE](https://drive.proton.me/urls/MZMDKRAXM0#DPBSHmy8ATHp)

The data is pre-processed using the following command, you need to run this command before training the model.
```bash
python src/data/preprocess.py
```

# Train

### Train a single model

model can be `135m`, `360m`, `1.7b`

```BASH 
python src/LLM/LLM_train.py --model "135m" 
```

`--sample` is used to train the model on a subset of the data, specify the number of samples you want to train on. 

```bash 
python src/LLM/LLM_train.py --model "1.7b" --sample=1000
```

the model will be saved in the `models` directory along with the tokenizer 

### Train all models
train all models 135m, 360m, 1.7b

```bash
python src/LLM/LLM_train.py --model "all" 
```

- `--sample` is used to train the model on a small subset of the data (specify the number of samples you want to train on)
- `--signature` is used to train the model on the signature data
- `--baseline` is used to train the model on the baseline data without any preprocessing

```bash
python src/LLM/LLM_train.py --model "all" --signature --sample=1000
```
```bash 
python src/LLM/LLM_train.py --model "all" --baseline --sample=1000
```
# Models download 
The models can be downloaded from the following links:
- [135m](https://1drv.ms/u/s!AjZv2fgaLt2FyKU_6pirCIoG5S-CWQ?e=MqcmZP)
- [360m](ADD link)
- [1.7b](ADD link)

# Evaluate
To use the evaluation script, you need to have the model in the `models` directory trained. 

### Evaluate a single model

```bash
python src/LLM/LLM_predict.py --prompt '"""A simple for loop"""' --model "135m"
```
- `--max_length` is used to specify the maximum number of tokens to the output.
- `--signature` is used to evaluate the model on the signature data.
- `--baseline` is used to evaluate the model on the baseline data without any preprocessing.
- `--original_output` do not apply any post-processing to the output.

### Evaluate all models
```bash
```bash
python src/LLM/LLM_predict.py --prompt '"""A simple for loop"""' --model "all"
```

```bash 
python src/LLM/LLM_predict.py --prompt '"""for i in 1:10"""' --model "all" --signature
```

```bash
python src/LLM/LLM_predict.py --prompt '"""for i in 1:10"""' --model "all" --baseline
```



# Chat bot UI
To run the chatbot UI

```bash
python src/chatbot/app.py
```
# Benchmark
## Generate
Go inside the benchmark directory
```bash
cd benchmark
```

### Generate response of models
Replace the name of the model with the one you want to run with benchmark, for example:
```bash
generate.sh  ../models/135m
```
evaluate.sh was customized to load our tokenizer.

The predictions generated by our Model are stored in `results` folder

### Evaluate
To use the evaluation script, you need to have the model in the `models` directory trained.
Replace `135m` with the desired model you want to evaluate and the checkpoint directory. 

```bash
evaluate.sh models/135m
```
The results are stored in a json file named $MODELNAME_results_jl.json. 

## Statistical Tests
Run the test.py in benchmark to get the efficiency of the models on MultiPl-E benchmark and some statistics.

```bash
python src/statistical.py
```

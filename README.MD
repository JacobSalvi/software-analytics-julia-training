


## Set up the environment
Create a virtual environment:

```bash
python3 -m venv .venv
```

Source the environment:
```bash
sourche .venv/bin/activawe
```

Source the environment on the server:
```bash
eval "$(/home/SA24-G1/miniconda3/bin/conda shell.bash hook)"
```


Install the requirements:
```bash
pip install -r requirements.txt
```

```bash
export PYTHONPATH="$PYTHONPATH:$PWD"
```
## Pre-Process the data 
The raw Data is expected to be in the data directory. The name of the file is expected to be `fd.json`
Download the raw data from [HERE](https://drive.proton.me/urls/MZMDKRAXM0#DPBSHmy8ATHp)
(TODO ADD ONEDRIVE DATA)

The data is pre-processed using the following command, you need to run this command before training the model.
```bash
python src/data/preprocess.py
```

## Train

### Train a single model

model can be `135m`, `360m`, `1.7b`

```BASH 
python src/LLM/LLM_train.py --model "360m" 
```

`--sample_run` is used to train the model on a small subset of the data
```bash 
python src/LLM/LLM_train.py --model "1.7b" --sample_run
```

the model will be saved in the `models` directory along with the tokenizer 


### Train all models
train all models 135m, 360m, 1.7b

```bash
python src/LLM/LLM_train.py --model "all" 
```

- `--sample_run` is used to train the model on a small subset of the data
- `--signature` is used to train the model on the signature data
- `--baseline` is used to train the model on the baseline data without any preprocessing

```bash
python src/LLM/LLM_train.py --model "all" --signature --sample_run
```
```bash 
python src/LLM/LLM_train.py --model "all" --baseline --sample_run
```

## Generate
To generate the predictions from the prompt first add tne path to benchmark to the environment.

#### Add the path of scripts to the environment
```bash
nano ~/.bashrc 
```
#### Add ~/benchmark then reload the environment
```bash
source ~/.bashrc
```

### Generate response of models
Replace the name of the model with the one you want to run with benchmark

```bash
generate.sh models/135m
```
The predictions generated by our Model are stored in `results` folder

## Evaluate
To use the evaluation script, you need to have the model in the `models` directory trained. 

### Evaluate a model
Replace `135m` with the desired model you want to evaluate.
```bash
evaluate.sh 135m
```
The results are stored in a json file named $MODELNAME_results_jl.json .

## Statistical Tests
Run the test.py in benchmark to get the efficiency of the models on MultiPl-E benchmark and some statistics.

```bash
python benchmark/test.py
```




## Chat bot UI
To run the chatbot UI

```bash
python src/chatbot/app.py
```
TO REMOVE: 

```bash 
python src/LLM/LLM_train.py --model "360m" --sample_run
```
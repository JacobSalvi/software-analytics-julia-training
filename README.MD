## Set up the environment
Create a virtual environment:

```bash
python3 -m venv .venv
```

Source the environment:
```bash
sourche .venv/bin/activawe
```

Source the environment on the server:
```bash
eval "$(/home/SA24-G1/miniconda3/bin/conda shell.bash hook)"
```


Install the requirements:
```bash
pip install -r requirements.txt
```

```bash
export PYTHONPATH="$PYTHONPATH:$PWD"
```
## Pre-Process the data 
The raw Data is expected to be in the data directory. The name of the file is expected to be `fd.json`
Download the raw data from [HERE](https://drive.proton.me/urls/MZMDKRAXM0#DPBSHmy8ATHp)
(TODO ADD ONEDRIVE DATA)

The data is pre-processed using the following command, you need to run this command before training the model.
```bash
python src/data/preprocess.py
```

## Train

### Train a single model

model can be `135m`, `360m`, `1.7b`

```BASH 
python src/LLM/LLM_train.py --model "135m" 
```

`--sample_run` is used to train the model on a small subset of the data
```bash 
python src/LLM/LLM_train.py --model "1.7b" --sample_run
```

the model will be saved in the `models` directory along with the tokenizer 


### Train all models
train all models 135m, 360m, 1.7b

```bash
python src/LLM/LLM_train.py --model "all" 
```

- `--sample_run` is used to train the model on a small subset of the data
- `--signature` is used to train the model on the signature data
- `--baseline` is used to train the model on the baseline data without any preprocessing

```bash
python src/LLM/LLM_train.py --model "all" --signature --sample_run
```
```bash 
python src/LLM/LLM_train.py --model "all" --baseline --sample_run
```

## Evaluate
To use the evaluation script, you need to have the model in the `models` directory trained. 

### Evaluate a single model

```bash
python src/LLM/LLM_predict.py --prompt '"""A simple for loop"""' --model "135m"
```
- `--signature` is used to evaluate the model on the signature data
- `--baseline` is used to evaluate the model on the baseline data without any preprocessing

### Evaluate all models

```bash
python src/LLM/LLM_predict.py --prompt '"""A simple for loop"""' --model "all"
```

```bash 
python src/LLM/LLM_predict.py --prompt '"""for i in 1:10"""' --model "all" --signature
```

```bash
python src/LLM/LLM_predict.py --prompt '"""for i in 1:10"""' --model "all" --baseline
```

## Chat bot UI
To run the chatbot UI

```bash
python src/chatbot/app.py
```

## Generate
To generate the predictions from the prompt first add tne path to benchmark to the environment.

```bash 
export PATH="/benchmark:$PATH"
```
## Benchmark
### Generate response of models
Replace the name of the model with the one you want to run with benchmark

```bash
generate.sh models/135m
```

The predictions generated by our Model are stored in `results` folder

### Evaluate
To use the evaluation script, you need to have the model in the `models` directory trained.
Replace `135m` with the desired model you want to evaluate.

```bash
evaluate.sh models/135m
```
The results are stored in a json file named $MODELNAME_results_jl.json .

## Statistical Tests
Run the test.py in benchmark to get the efficiency of the models on MultiPl-E benchmark and some statistics.

```bash
python benchmark/test.py
```

```bash 
python src/LLM/LLM_train.py --model "1.7b" --sample_run
```
